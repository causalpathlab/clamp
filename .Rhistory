<<<<<<< Updated upstream

load_all()
res_cl3 <- clamp(X, y, W=Wmat,
standardize = F,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl3, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl3)
res_cl2 <- clamp(X, y, W=Wmat,
standardize = T,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl2)
l <- 1
plot(res_cl3$alpha[l,], res_cl2$alpha[l,])
l <- 2
plot(res_cl3$alpha[l,], res_cl2$alpha[l,])
prior_varB <- var(y) * 0.2
post_varB_cl2 <- res_cl2$mu2 - res_cl2$mu^2
shat2_cl2 <- 1/(1/post_varB_cl2 - 1/prior_varB)
shat2_cl2 <- 1/sweep(1/post_varB_cl2, 1, 1/prior_varB, "-"))
shat2_cl2 <- 1/sweep(1/post_varB_cl2, 1, 1/prior_varB, "-")
shat2_cl2 <- 1/sweep(1/post_varB_cl2, 2, 1/prior_varB, "-")
1/prior_varB
1/post_varB_cl2
post_varB_cl3 <- res_cl3$mu2 - res_cl3$mu^2
shat2_cl3 <- 1/sweep(1/post_varB_cl2, 2, 1/prior_varB, "-")
plot(shat2_cl2, shat2_cl3)
abline(0,1)
shat2_cl3 <- 1/sweep(1/post_varB_cl3, 2, 1/prior_varB, "-")
plot(shat2_cl2, shat2_cl3)
abline(0,1)
load_all()
res_cl3 <- clamp(X, y, W=Wmat,
standardize = F,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl3, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl3)
res_cl2 <- clamp(X, y, W=Wmat,
standardize = T,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl2)
rm(shat2_cl2)
rm(shat2_cl3)
rm(alpha_scaled, alpha_unscaled)
rm(bb_scaled, bb_unscaled)
l <- 1
plot(res_cl3$alpha[l,], res_cl2$alpha[l,])
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
l <- 2
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
l <- 3
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
l <- 4
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
summary(lm(res_cl3$shat2[1,] ~ res_cl2$shat2[1,]))
summary(lm(res_cl3$shat2[2,] ~ res_cl2$shat2[2,]))
load_all()
res_cl3 <- clamp(X, y, W=Wmat,
standardize = F,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl3, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl3)
res_cl2 <- clamp(X, y, W=Wmat,
standardize = T,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl2)
load_all()
res_cl3 <- clamp(X, y, W=Wmat,
standardize = F,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl3, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl3)
res_cl2 <- clamp(X, y, W=Wmat,
standardize = T,
mle_estimator = "WLS",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
clamp_plot(res_cl2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl2)
load_all()
plot(res_cl3$betahat[1,], res_cl2$betahat[1,])
plot(res_cl3$betahat[2,], res_cl2$betahat[2,])
plot(res_cl3$betahat[3,], res_cl2$betahat[3,])
plot(res_cl3$betahat[1,] / res_cl2$betahat[1,])
plot(res_cl3$betahat[2,] / res_cl2$betahat[2,])
plot(res_cl3$betahat[1,], res_cl2$betahat[1,])
abline(0, 0.5)
plot(res_cl2$betahat[1,]/ res_cl3$betahat[1,])
plot(res_cl2$betahat[1,]/ res_cl3$betahat[1,], sapply(1:ncol(W), function(j) weightedSd(y,W[,j])))
plot(res_cl2$betahat[1,]/ res_cl3$betahat[1,], sapply(1:ncol(W), function(j) weightedSd(X[,j],W[,j])))
abline(0,1)
l <- 1
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
l <- 1
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
l <- 1
plot(res_cl3$shat2[l,]*0.025, res_cl2$shat2[l,])
abline(0,1)
zscore2_cl3 <- res_cl3$betahat^2 / res_cl3$shat2
zscore2_cl2 <- res_cl2$betahat^2 / res_cl2$shat2
plot(zscore2_cl3, zscore2_cl2)
summary(lm(c(zscore2_cl3) ~ c(zscore2_cl2)))
l <- 1
plot(res_cl3$shat2[l,]*0.25, res_cl2$shat2[l,])
# abline(0,1)
abline(0,1)
l <- 1
plot(res_cl3$shat2[l,]*0.25, res_cl2$shat2[l,])
abline(0,1)
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
abline(0,1)
plot(res_cl3$shat2[l,], res_cl2$shat2[l,])
abline(0,1)
plot(c(res_cl3$betahat), c(res_cl2$betahat))
summary(lm(c(res_cl3$betahat) ~ c(res_cl2$betahat)))
summary(lm(c(res_cl3$shat2) ~ c(res_cl2$shat2)))
plot(c(res_cl3$shat2), c(res_cl2$shat2))
set.seed(107108)
nn <- 100
pp <- 2
h2 <- 1
U <- rnorm(nn)
zeta <- ( (1:pp) - (pp+1)/2 ) / (pp+1)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]
Pmat <- expit(UU)  # Prob matrix
X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)
esp <- rnorm(nn)
causal_vars <- sample.int(pp, size = 1)
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs) + sqrt(1-h2) * esp
zeta
zeta <- ( (1:pp) - pp/2 ) / (pp+1)
zeta
# zeta <- ( (1:pp) - pp/2 ) / (pp+1)
zeta <- rnorm(pp)
# zeta <- ( (1:pp) - pp/2 ) / (pp+1)
zeta <- rnorm(pp, sd = 2)
zeta
causal_vars
##
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ U, family = binomial), type = "response"))
Wmat <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(Wmat)
res_lm1 <- lm(y ~ X, weights = Wmat)
rm(Wmat)
X_ <- sapply(1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]))
W <- ifelse(X == 1, 1/PS, 1/(1-PS))
X_ <- sapply(1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]))
y_ <- sapply(1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
######################
## Q1: How standardization affects the bootstrap variances?
nboot <- 100
betahat
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
}
boot_var <- colVars(boot_betahat)
boot_var
res_lm1 <- lm(y ~ X[,1], weights = W[,1])
summary(res_lm1)
res_lm2 <- lm(y ~ X[,2], weights = W[,2])
summary(res_lm2)
coefficients(res_lm1)
coefint(res_lm1)
confint(res_lm1)
summary(res_lm1)$coefficients
summary(res_lm1)$coefficients[2,1:2]
res_lm2 <- lm(y ~ X[,2], weights = W[,2])
summary(res_lm2)$coeffcients[2,1:2]
summary(res_lm2)$coefficients[2,1:2]
betahat_exact <- matrix(NA, nrow = ncol(X), ncol = 2,
dimnames = list(paste0("X", 1:ncol(X)),
c("Estimate", "SE")))
betahat_exact
for (j in 1 : ncol(X)) {
res_lm <- lm(y ~ X[,j], weights = W[,j])
betahat_exact <- summary(res_lm)$coefficients[2,1:2]
}
betahat_exact
for (j in 1 : ncol(X)) {
res_lm <- lm(y ~ X[,j], weights = W[,j])
betahat_exact[j, ] <- summary(res_lm)$coefficients[2,1:2]
}
betahat_exact <- matrix(NA, nrow = ncol(X), ncol = 2,
dimnames = list(paste0("X", 1:ncol(X)),
c("Estimate", "SE")))
for (j in 1 : ncol(X)) {
res_lm <- lm(y ~ X[,j], weights = W[,j])
betahat_exact[j, ] <- summary(res_lm)$coefficients[2,1:2]
}
betahat_exact
betahat_bs1 <- matrix(t(cbind(betahat, sqrt(boot_var))))
betahat_bs
betahat_bs1
betahat_bs1 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs1
betahat_exact <- data.frame(Estimate = rep(NA, times = ncol(X)),
SE = rep(NA, times = ncol(X)),
row.names = paste0("X", 1:ncol(X)))
for (j in 1 : ncol(X)) {
res_lm <- lm(y ~ X[,j], weights = W[,j])
betahat_exact[j, ] <- summary(res_lm)$coefficients[2,1:2]
}
betahat_exact <- as.data.frame(betahat_exact)
betahat_exact
betahat_bs1
X_ <- sapply( 1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]) )
X_ <- sapply( 1:ncol(W), function(j) X_[,j] / weightedSd(X[,j], W[,j]) )
y_ <- sapply( 1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot_[,j] / weightedSd(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
}
boot_var <- colVars(boot_betahat)
betahat_bs2 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs2
## convert back to the original scale
XcolSds <- sapply( 1:ncol(W), function(j) weightedSd(X[,j], W[,j]))
XcolSds
?sweep
A <- array(1:24, dim = 4:2)
A
sweep(A, 1, 5)
sweep(betahat_bs2, 2, XcolSds, "/")
betahat_bs1
set.seed(107108109)
nn <- 100
pp <- 2
h2 <- 1
U <- rnorm(nn)
zeta <- rnorm(pp)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]
Pmat <- expit(UU)  # Prob matrix
X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)
esp <- rnorm(nn)
causal_vars <- 1
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs) +
sqrt(1-h2) * esp
##
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ U, family = binomial), type = "response"))
W <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(W)
betahat_exact <- data.frame(Estimate = rep(NA, times = ncol(X)),
SE = rep(NA, times = ncol(X)),
row.names = paste0("X", 1:ncol(X)))
for (j in 1 : ncol(X)) {
res_lm <- lm(y ~ X[,j], weights = W[,j])
betahat_exact[j, ] <- summary(res_lm)$coefficients[2,1:2]
}
betahat_exact
### Bootstrap with Unstandardized X
nboot <- 100
X_ <- sapply( 1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]) )
y_ <- sapply( 1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
}
boot_var <- colVars(boot_betahat)
betahat_bs1 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs1
X_ <- sapply( 1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]) )
X_ <- sapply( 1:ncol(W), function(j) X_[,j] / weightedSd(X[,j], W[,j]) )
y_ <- sapply( 1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot_[,j] / weightedSd(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
}
boot_var <- colVars(boot_betahat)
betahat_bs2 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs2
#### convert back to the original scale
XcolSds <- sapply( 1:ncol(W), function(j) weightedSd(X[,j], W[,j]))
sweep(betahat_bs2, 2, XcolSds, "/")
betahat_bs1
betahat_exact
set.seed(107108109110)
set.seed(107110)
nn <- 1000  ## 10 times the sample size of Q1
pp <- 2
h2 <- 1
U <- rnorm(nn)
zeta <- rnorm(pp)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]
Pmat <- expit(UU)  # Prob matrix
X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)
esp <- rnorm(nn)
causal_vars <- 1
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs) +
sqrt(1-h2) * esp
##
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ U, family = binomial), type = "response"))
W <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(W)
### Bootstrap with Unstandardized X
nboot <- 100
X_ <- sapply( 1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]) )
y_ <- sapply( 1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
betahat
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
=======
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
=======
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
>>>>>>> Stashed changes
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
plot(res_cl$loglik, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
plot_select_results(res_cl$variable_pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
level_names <- paste0(rep(1:10, each = 2), "_", 1:2)
plot_select_results(res_cl$level_pip, (2*effect_ind-1):(2*effect_ind)) +
scale_x_continuous(breaks = seq(1:20),
labels = paste0("X", level_names)) +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
ylab("level-wise PIP") +
xlab("variable_level")
resY <- residuals(lm(Y ~ U))
res_su <- susieR::susie(X_original, resY, L = 5)
plot(res_su$pip)
susieR::summary.susie(res_su)
plot_select_results(res_su$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_su0 <- susieR::susie(X_original, Y, L = 5)
plot(res_su0$pip)
susieR::summary.susie(res_su0)
plot_select_results(res_su0$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_la <- glmnet::cv.glmnet(X_original, Y, family = "gaussian", alpha = 1)
plot_select_results(abs(as.numeric(coef(res_la, s = "lambda.min"))[-1]),
effect_idx = effect_ind) +
ylab("|coefficients|")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
devtools::load_all(".")
# library(clamp)
library(nnet)
library(data.table)
data.dir <- "~/Documents/research/data/genotype/"
## for plots
library(tidyverse)
myPalette2 <- c("#E69F00", "#999999", "#00757F")
myPalette3 <- c("dodgerblue2", "green4", "#6A3D9A", "#FF7F00",
"gold1", "skyblue2", "#FB9A99", "palegreen2", "#CAB2D6",
"#FDBF6F", "gray70", "khaki2", "maroon", "orchid1", "deeppink1",
"blue1", "steelblue4", "darkturquoise", "green1", "yellow4",
"yellow3", "darkorange4", "brown")
plot_select_results <- function(vals, effect_idx, cs_list = NULL) {
tibble(vals = vals) %>%
mutate(id = 1 : n()) %>%
mutate(true_label = if_else(id %in% effect_idx, "causal", "noncausal")) %>%
arrange(desc(true_label)) %>%
ggplot(aes(x = id, y = vals, color = as.factor(true_label))) +
geom_point(size = 3) +
scale_color_manual(values = myPalette2[1:2]) +
labs(x = "variable", color = "") +
theme_classic()
}
# tab <- tibble(vals = vals) %>%
#   mutate(id = 1 : n(), cs = -1) %>%
#   mutate(true_label = if_else(id %in% effect_idx, "causal", "noncausal")) %>%
#   arrange(desc(true_label))
#
# for (i in seq_along(cs_list)) {
#   tab$cs[tab$id %in% cs_list[[i]]] <- i
# }
#
# ggplot(data = tab) +
#   geom_point(aes(x = id, y = vals,
#                  color = as.factor(true_label),
#                  shape = factor(cs)), size = 3) +
#   scale_color_manual(values = myPalette2[1:2]) +
#   scale_shape_manual()
set.seed(123)
n <- 1000
p <- 20
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.2)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## Delta1 = 2; Delta2 = 4
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(4)
# Y <-
#   X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
#   X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
#   X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
#   X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
#   X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
#   X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) + eps
# X[,(effect_ind[3] - 1) * K + 1] * f3func(0, U) +
# X[,(effect_ind[3] - 1) * K + 2] * f3func(1, U) +
# X[,(effect_ind[3] - 1) * K + 3] * f3func(2, U) + eps
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + U + eps
as.matrix(rep(1, times = length(effect_ind))) + 5*U + eps
## Robust approach 1: IPW truncation
.clipped <- function(.val, .alpha = 0.05) {
ifelse(.val < .alpha, .alpha,
ifelse(.val > (1-.alpha), 1-.alpha, .val))
}
boot_var <- colVars(boot_betahat)
betahat_bs1 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs3 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs3
set.seed(107108109)
nn <- 100
pp <- 2
h2 <- 1
U <- rnorm(nn)
zeta <- rnorm(pp)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]
Pmat <- expit(UU)  # Prob matrix
X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)
esp <- rnorm(nn)
causal_vars <- 1
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs) +
sqrt(1-h2) * esp
##
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ U, family = binomial), type = "response"))
W <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(W)
### Bootstrap with Unstandardized X
nboot <- 100
X_ <- sapply( 1:ncol(W), function(j) X[,j] - weighted.mean(X[,j], W[,j]) )
y_ <- sapply( 1:ncol(W), function(j) y - weighted.mean(y, W[,j]) )
wxy <- colSums(W * X_ * y_)
wx2  <- colSums(W * X_^2)
betahat <- wxy / wx2
boot_betahat <- matrix(NA, nrow = nboot, ncol = ncol(X))
for (B in 1 : nboot) {
ind <- sample.int(nrow(X), size = nrow(X), replace = T)
Xboot <- X[ind, , drop=F]
yboot <- y[ind]
Wboot <- W[ind, , drop=F]
Xboot_ <- sapply(1:ncol(Wboot),
function(j) Xboot[,j] - weighted.mean(Xboot[,j], Wboot[,j]))
yboot_ <- sapply(1:ncol(Wboot),
function(j) yboot - weighted.mean(yboot, Wboot[,j]))
wxy <- colSums(Wboot * Xboot_ * yboot_)
wx2 <- colSums(Wboot * Xboot_^2)
boot_betahat[B, ] <- wxy / wx2
}
<<<<<<< Updated upstream
boot_var <- colVars(boot_betahat)
betahat_bs1 <- data.frame(Estimate = betahat, SE = sqrt(boot_var))
betahat_bs1
betahat_bs3
betahat
betahat / XcolSds
boot_betahat
sweep(boot_betahat, 2, XcolSds, "/")
sweep(boot_betahat, 2, c(0, 1), "*")
load_all()
rm(list = ls())
set.seed(107108)
nn <- 100
# pp <- 1000
pp <- 10
h2 <- 1
U <- rnorm(nn)
zeta <- ( (1:pp) - pp/2 ) / (pp+1)
# zeta <- rnorm(pp, sd = 1)
# zeta <- rep(0, times = pp)
# hist(zeta)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]
Pmat <- expit(UU)  # Prob matrix
# hist(cor(Pmat), breaks = seq(-1, 1, by = 0.1))
# corrplot(cor(Pmat), type = "upper")
X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)
esp <- rnorm(nn)
causal_vars <- sample.int(pp, size = 5)
# coefs <- rnorm(length(causal_vars) + 1)
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs) + sqrt(1-h2) * esp
print(data.frame(variable = c(paste0("X", causal_vars), "U"),
effect_size = coefs))
Uhat <- rsvd(X, k=3)$u
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ Uhat, family = binomial), type = "response"))
Uhat <- rsvd(X, k=3)$u
PS <- sapply(1:ncol(X),
function(j) predict(glm(X[,j] ~ Uhat, family = binomial), type = "response"))
zeta
Wmat <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(Wmat)
res_cl1 <- clamp(X, y, W=Wmat,
standardize = F,
mle_estimator = "mHT",
max_iter = 1,
maxL = 10,
seed = 123,
verbose = T)
=======
colnames(ProbPred) <- colnames(X)
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
# par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
# for (j in 1 : ncol(X_original)) {
#   plot(X_original[,j], ProbPred_integrated[,j],
#        xlab = paste0("X", j), ylab = paste0("estimated propensity"))
# }
W <- 1 / .clipped(ProbPred)
# W <- 1 / ProbPred * .overlap_weighting_g(ProbPred)
# W <- 1 / ProbPred * .entropy_weighting_g(ProbPred)
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
plot(res_cl$loglik, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
plot_select_results(res_cl$variable_pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
level_names <- paste0(rep(1:10, each = 2), "_", 1:2)
plot_select_results(res_cl$level_pip, (2*effect_ind-1):(2*effect_ind)) +
scale_x_continuous(breaks = seq(1:20),
labels = paste0("X", level_names)) +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
ylab("level-wise PIP") +
xlab("variable_level")
resY <- residuals(lm(Y ~ U))
res_su <- susieR::susie(X_original, resY, L = 5)
plot(res_su$pip)
susieR::summary.susie(res_su)
plot_select_results(res_su$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_su0 <- susieR::susie(X_original, Y, L = 5)
plot(res_su0$pip)
susieR::summary.susie(res_su0)
plot_select_results(res_su0$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_la <- glmnet::cv.glmnet(X_original, Y, family = "gaussian", alpha = 1)
plot_select_results(abs(as.numeric(coef(res_la, s = "lambda.min"))[-1]),
effect_idx = effect_ind) +
ylab("|coefficients|")
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + 5*U + eps
set.seed(123)
n <- 500
p <- 20
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.2)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## Delta1 = 2; Delta2 = 4
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(4)
# Y <-
#   X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
#   X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
#   X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
#   X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
#   X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
#   X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) + eps
# X[,(effect_ind[3] - 1) * K + 1] * f3func(0, U) +
# X[,(effect_ind[3] - 1) * K + 2] * f3func(1, U) +
# X[,(effect_ind[3] - 1) * K + 3] * f3func(2, U) + eps
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + 5*U + eps
## Robust approach 1: IPW truncation
.clipped <- function(.val, .alpha = 0.05) {
ifelse(.val < .alpha, .alpha,
ifelse(.val > (1-.alpha), 1-.alpha, .val))
}
## Robust approach 2: balancing weight
### Option 1: overlap weighting
.overlap_weighting_g <- function(.e) { .e * (1 - .e) }
### Option 2: Entropy weighting
.entropy_weighting_g <- function(.e) {-(.e * .loge(.e) + (1-.e) * .loge(1-.e) )}
## estimated confounder
# Uhat <- rsvd::rsvd(X_original, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
# mn.fit <- multinom(Xj ~ Uhat)
mn.fit <- multinom(Xj ~ U)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
# par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
# for (j in 1 : ncol(X_original)) {
#   plot(X_original[,j], ProbPred_integrated[,j],
#        xlab = paste0("X", j), ylab = paste0("estimated propensity"))
# }
W <- 1 / .clipped(ProbPred)
# W <- 1 / ProbPred * .overlap_weighting_g(ProbPred)
# W <- 1 / ProbPred * .entropy_weighting_g(ProbPred)
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
<<<<<<< Updated upstream
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
colnames(ProbPred) <- colnames(X)
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
# par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
# for (j in 1 : ncol(X_original)) {
#   plot(X_original[,j], ProbPred_integrated[,j],
#        xlab = paste0("X", j), ylab = paste0("estimated propensity"))
# }
W <- 1 / .clipped(ProbPred)
# W <- 1 / ProbPred * .overlap_weighting_g(ProbPred)
# W <- 1 / ProbPred * .entropy_weighting_g(ProbPred)
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
=======
>>>>>>> Stashed changes
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
plot(res_cl$loglik, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
plot_select_results(res_cl$variable_pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
level_names <- paste0(rep(1:10, each = 2), "_", 1:2)
plot_select_results(res_cl$level_pip, (2*effect_ind-1):(2*effect_ind)) +
scale_x_continuous(breaks = seq(1:20),
labels = paste0("X", level_names)) +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
ylab("level-wise PIP") +
xlab("variable_level")
resY <- residuals(lm(Y ~ U))
res_su <- susieR::susie(X_original, resY, L = 5)
plot(res_su$pip)
susieR::summary.susie(res_su)
plot_select_results(res_su$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_su0 <- susieR::susie(X_original, Y, L = 5)
plot(res_su0$pip)
susieR::summary.susie(res_su0)
plot_select_results(res_su0$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_la <- glmnet::cv.glmnet(X_original, Y, family = "gaussian", alpha = 1)
plot_select_results(abs(as.numeric(coef(res_la, s = "lambda.min"))[-1]),
effect_idx = effect_ind) +
ylab("|coefficients|")
<<<<<<< Updated upstream
=======
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + 5*U + eps
set.seed(123)
n <- 500
p <- 20
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.2)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## Delta1 = 2; Delta2 = 4
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(4)
# Y <-
#   X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
#   X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
#   X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
#   X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
#   X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
#   X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) + eps
# X[,(effect_ind[3] - 1) * K + 1] * f3func(0, U) +
# X[,(effect_ind[3] - 1) * K + 2] * f3func(1, U) +
# X[,(effect_ind[3] - 1) * K + 3] * f3func(2, U) + eps
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + 5*U + eps
## Robust approach 1: IPW truncation
.clipped <- function(.val, .alpha = 0.05) {
ifelse(.val < .alpha, .alpha,
ifelse(.val > (1-.alpha), 1-.alpha, .val))
}
## Robust approach 2: balancing weight
### Option 1: overlap weighting
.overlap_weighting_g <- function(.e) { .e * (1 - .e) }
### Option 2: Entropy weighting
.entropy_weighting_g <- function(.e) {-(.e * .loge(.e) + (1-.e) * .loge(1-.e) )}
## estimated confounder
# Uhat <- rsvd::rsvd(X_original, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
# mn.fit <- multinom(Xj ~ Uhat)
mn.fit <- multinom(Xj ~ U)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
# par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
# for (j in 1 : ncol(X_original)) {
#   plot(X_original[,j], ProbPred_integrated[,j],
#        xlab = paste0("X", j), ylab = paste0("estimated propensity"))
# }
W <- 1 / .clipped(ProbPred)
# W <- 1 / ProbPred * .overlap_weighting_g(ProbPred)
# W <- 1 / ProbPred * .entropy_weighting_g(ProbPred)
# par(mfrow = c(3, 4))
# for (j in 1 : ncol(X_original)) {
#   # plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("OW"))
#   plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
#        xlab = paste0("X", j), ylab = paste0("OW"))
#   # plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#   #      xlab = paste0("X", j), ylab = paste0("EW"))
# }
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
plot(res_cl$loglik, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
plot_select_results(res_cl$variable_pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
level_names <- paste0(rep(1:10, each = 2), "_", 1:2)
plot_select_results(res_cl$level_pip, (2*effect_ind-1):(2*effect_ind)) +
scale_x_continuous(breaks = seq(1:20),
labels = paste0("X", level_names)) +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
ylab("level-wise PIP") +
xlab("variable_level")
resY <- residuals(lm(Y ~ U))
res_su <- susieR::susie(X_original, resY, L = 5)
plot(res_su$pip)
susieR::summary.susie(res_su)
plot_select_results(res_su$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_su0 <- susieR::susie(X_original, Y, L = 5)
plot(res_su0$pip)
susieR::summary.susie(res_su0)
plot_select_results(res_su0$pip, effect_ind) +
scale_x_continuous(breaks = seq(1, 10),
labels = paste0("X", as.character(1:10))) +
ylab("variable-wise PIP")
res_la <- glmnet::cv.glmnet(X_original, Y, family = "gaussian", alpha = 1)
plot_select_results(abs(as.numeric(coef(res_la, s = "lambda.min"))[-1]),
effect_idx = effect_ind) +
ylab("|coefficients|")
>>>>>>> Stashed changes
# Function for expit
expit <- function(x) ifelse(x > 0, 1 / (1 + exp(-x)), exp(x) / (1 + exp(x)))
# Load libraries
library(ggplot2)
library(reshape2)
set.seed(123)
# Parameters
n <- 500   # number of individuals
p <- 200    # number of treatments
# Simulate latent confounder U
u <- rnorm(n, mean = 0, sd = 1)
# Function for expit
expit <- function(x) ifelse(x > 0, 1 / (1 + exp(-x)), exp(x) / (1 + exp(x)))
# Simulate propensity scores matrix (n x p)
E <- matrix(NA, nrow = n, ncol = p)
for (j in 1:p) {
eps <- rnorm(n, mean = 0, sd = 0.1)
linear_pred <- ((j - 0.5 * p) / (p + 1)) * u + eps
E[, j] <- expit(linear_pred)
}
colnames(E) <- paste0("X", 1:p)
# Correlation matrix
cor_mat <- cor(E)
# Convert to long format for ggplot
cor_df <- melt(cor_mat)
colnames(cor_df) <- c("Var1", "Var2", "Correlation")
# Correlation heatmap
ggplot(cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", mid = "white", high = "red",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
coord_fixed() +
labs(title = "Correlation Plot of Propensity Scores",
x = "Treatment Variables",
y = "Treatment Variables")
corrplot::corrplot(cor_mat)
ggplot(cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", mid = "white", high = "red",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
coord_fixed() +
labs(title = "Correlation Plot of Propensity Scores",
x = "Treatment Variables",
y = "Treatment Variables")
# Correlation heatmap
ggplot(cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", mid = "white", high = "red",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()) +
coord_fixed() +
labs(title = "Correlation Plot of Propensity Scores",
x = "Treatment Variables",
y = "Treatment Variables")
# Correlation heatmap
ggplot(cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", mid = "white", high = "red",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()) +
coord_fixed() +
labs(#title = "Correlation plot of Propensity Scores",
x = "Treatment Variables (Var 1)",
y = "Treatment Variables (Var 2)")
# Correlation heatmap
ggplot(cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", mid = "white", high = "red",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()) +
coord_fixed() +
labs(#title = "Correlation plot of Propensity Scores",
x = "Treatment Variables",
y = "Treatment Variables")
cor_mat[c(1, 2, 31, 65, 192), c(1, 2, 31, 65, 192)]
