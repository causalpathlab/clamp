}
colnames(ProbPred) <- colnames(X)
ProbPred <- .clipped(ProbPred)
W <- 1 / ProbPred
res_non_hierarchical <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 50,
seed = 123,
verbose = T)
effect_ind
res_non_hierarchical <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 50,
seed = 123,
verbose = T)
plot(res_non_hierarchical$level_pip)
plot(res_non_hierarchical$variable_pip)
plot(res_non_hierarchical$elbo, type = "o")
clamp_summarize_coefficients(res_non_hierarchical)
summary(res_non_hierarchical)
clamp_summarize_coefficients(res_non_hierarchical)
res_cl <- res_non_hierarchical
rm(res_non_hierarchical)
res_cl$alpha
round(res_cl$alpha, 2)
res_cl$prior_varD
res_cl$logBF_model
res_cl$logBF
corrplot::corrplot(cor(X_original), method = 'number')
Y <-
X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) +
X[,(effect_ind[3] - 1) * K + 1] * f2func(0, U) +
X[,(effect_ind[3] - 1) * K + 2] * f2func(1, U) +
X[,(effect_ind[3] - 1) * K + 3] * f2func(2, U) + eps
effect_ind
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 50,
seed = 123,
verbose = T)
use_r("elbo")
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 25,
seed = 123,
verbose = T)
res_cl$prior_varD
log(sqrt(10))
log(10)
res_cl$logBF
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
effect_ind <- sample(1:p, size = 3)
set.seed(123)
n <- 1000
p <- 10
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.5)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## (Delta1 = 1; Delta2 = 2)*2
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(1, 2, 5)
Y <-
X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) +
X[,(effect_ind[3] - 1) * K + 1] * f3func(0, U) +
X[,(effect_ind[3] - 1) * K + 2] * f3func(1, U) +
X[,(effect_ind[3] - 1) * K + 3] * f3func(2, U) + eps
## estimated confounder
Uhat <- rsvd::rsvd(X_original, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
mn.fit <- multinom(Xj ~ Uhat)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
ProbPred <- .clipped(ProbPred)
W <- 1 / ProbPred
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
for (j in 1 : ncol(X)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
par(mfrow = c(3, 3))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
par(mfrow = c(3, 3))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
res_cl$elbo - res_cl$loglik
plot(res_cl$elbo - res_cl$loglik)
plot(res_cl$elbo - res_cl$loglik)
par(mfrow = c(1, 1))
plot(res_cl$elbo - res_cl$loglik)
plot(res_cl$elbo - res_cl$loglik, type = "o")
plot(res_cl$loglik, type = "o")
plot(res_cl$loglik[-1], type = "o")
plot(res_cl$elbo, type = "o")
plot(res_cl$elbo[-1], type = "o")
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 17,
seed = 123,
verbose = T)
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 17,
seed = 123,
verbose = T)
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 50,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
res_cl$prior_varD
res_cl$logBF
log(10)
log(sqrt(10))
colSums(X)
round(res_cl$alpha, 2)
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 10,
nboots = 1000,
seed = 123,
verbose = T)
set.seed(123)
n <- 1000
p <- 10
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.5)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## (Delta1 = 1; Delta2 = 2)*2
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(1, 2, 5)
Y <-
X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) +
X[,(effect_ind[3] - 1) * K + 1] * f2func(0, U) +
X[,(effect_ind[3] - 1) * K + 2] * f2func(1, U) +
X[,(effect_ind[3] - 1) * K + 3] * f2func(2, U) + eps
## estimated confounder
Uhat <- rsvd::rsvd(X_original, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
mn.fit <- multinom(Xj ~ Uhat)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
ProbPred <- .clipped(ProbPred)
W <- 1 / ProbPred
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
par(mfrow = c(3, 3))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 30,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
round(res_cl$alpha)
round(res_cl$alpha, 2)
res_su <- susieR::susie(X_original, Y)
res_su <- susieR::susie(X_original, Y, max_iter = 500)
res_su$pip
round(res_cl$alpha*res_cl$mu, 2)
round(res_cl$alpha*res_cl$mu, 1)
round(res_su$alpha*res_su$mu, 1)
## estimated confounder
Uhat <- rsvd::rsvd(X, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
mn.fit <- multinom(Xj ~ Uhat)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
ProbPred <- .clipped(ProbPred)
W <- 1 / ProbPred
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
par(mfrow = c(3, 3))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
max_iter = 30,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
devtools::load_all(".")
# library(clamp)
library(nnet)
library(data.table)
data.dir <- "~/Documents/research/data/genotype/"
set.seed(12345)
n <- 1000
p <- 10
K <- 3  ## 3-level categorical variable.
# confounder
U <- rnorm(n, mean = 0.5)
# potential outcomes
eps <- rnorm(n)
# When mean(U) = 0.5
# ============ f1 ============
f1func <- function(.x, .u) return(.x + .u + .x*.u)
## E[f1(0,U)] = 0.5; E[f1(1,U)] = 2; E[f1(2,U)] = 3.5
## Delta1 = 1.5; Delta2 = 3
# ============ f2 ============
f2func <- function(.x, .u) return(2*(.x - 0.8*.u))
## (E[f2(0,U)] = -0.4; E[f2(1,U)] = 0.6; E[f2(2,U)] = 1.6)*2
## Delta1 = 2; Delta2 = 4
# ============ f3 ============
f3func <- function(.x, .u) return(-.x^2 + .u)
## E[f3(0,U)] = 0.5, E[f3(1,U)] = -0.5, E[f3(2,U)] = -3.5
## (Delta1 = -1, Delta2 = -4)
# treatment assignment: multinomial logistic regression (softmax)
## .lp: matrix of linear predictors (s). It should contain (K-1) columns, and each column is an n-vector of the linear predictors of the k-th level (1 <= k < K)
softmax <- function(.lp) {
K <- length(.lp) + 1  ## number of levels
exp.s <- apply(.lp, 2, exp)  ## exp(LinearPredictor), n by (K-1)
Z <- rowSums(exp.s) + 1  ## denominator
prob.mat <- cbind(1, exp.s) / Z
return(prob.mat)
}
xi.matrix <- matrix(nrow = 2, ncol = p*(K-1))
# 1st row: intercepts
# 2nd row: slopes of U
intercepts_base <- 0
slopes <- seq(from = -10, to = -2, length.out = p*(K-1))
slopes_eps <- rnorm(n=length(slopes), sd = 0.1)
xi.matrix[1,] <- intercepts_base
xi.matrix[2,] <- slopes + slopes_eps
LinearPred <- apply(xi.matrix, 2, function(x) x[1] + x[2]*U)
X <- matrix(NA)
for (j in 1 : p) {
probs <- softmax(LinearPred[, ((j-1)*(K-1)+1) : (j*(K-1)) , drop=F])
Xj <- t(apply(probs, 1, function(pr) {rmultinom(1, 1, prob = pr)})) ## n by K
if (sum(is.na(X))) {
X <- Xj
} else {
X <- cbind(X, Xj)
}
}
colnames(X) <- paste0(rep(paste0("X", 1:p, "_"), each = K), 0:(K-1))
X <- apply(X, 2, as.double)
## Turn it into a n by p matrix with entries 0, 1, 2
X_original <- sweep(X, 2, as.vector(rep(1:K, times = p)), "*")
variable_names <- sub("_.*", "", colnames(X_original))
X_original <- sapply(unique(variable_names), function(v) {
rowSums(X_original[, variable_names == v, drop=F])
})
X_original <- X_original - 1
head(X_original)
corrplot::corrplot(cor(X_original), method = 'number')
############################################
# observed outcome
# effect_ind <- sample(1:p, size = 3)
effect_ind <- c(5)
# Y <-
#   X[,(effect_ind[1] - 1) * K + 1] * f1func(0, U) +
#   X[,(effect_ind[1] - 1) * K + 2] * f1func(1, U) +
#   X[,(effect_ind[1] - 1) * K + 3] * f1func(2, U) +
#   X[,(effect_ind[2] - 1) * K + 1] * f2func(0, U) +
#   X[,(effect_ind[2] - 1) * K + 2] * f2func(1, U) +
#   X[,(effect_ind[2] - 1) * K + 3] * f2func(2, U) + eps
# X[,(effect_ind[3] - 1) * K + 1] * f3func(0, U) +
# X[,(effect_ind[3] - 1) * K + 2] * f3func(1, U) +
# X[,(effect_ind[3] - 1) * K + 3] * f3func(2, U) + eps
Y <- X_original[,effect_ind, drop=F] %*%
as.matrix(rep(1, times = length(effect_ind))) + U + eps
## Robust approach 1: IPW truncation
.clipped <- function(.val, .alpha = 0.05) {
ifelse(.val < .alpha, .alpha,
ifelse(.val > (1-.alpha), 1-.alpha, .val))
}
## Robust approach 2: balancing weight
### Option 1: overlap weighting
.overlap_weighting_g <- function(.e) { .e * (1 - .e) }
### Option 2: Entropy weighting
.entropy_weighting_g <- function(.e) {-(.e * .loge(.e) + (1-.e) * .loge(1-.e) )}
## estimated confounder
# Uhat <- rsvd::rsvd(X_original, k = 3)$u
# require(nnet)
ProbPred <- matrix(nrow = nrow(X), ncol = ncol(X)) ## same size as X
col_indices <- colnames(X)
for (j in 1 : p) {
jcols <- grepl(paste0("X", as.character(j), "_"), col_indices)
Xj <- X[, jcols, drop=F]
# mn.fit <- multinom(Xj ~ Uhat)
mn.fit <- multinom(Xj ~ U)
ProbPred[, jcols] <- predict(mn.fit, type = "probs")
}
colnames(ProbPred) <- colnames(X)
## Check the propensities in the specific levels
ProbPred_integrated <- X * ProbPred
variable_names <- sub("_.*", "", colnames(X))
ProbPred_integrated <- sapply(unique(variable_names), function(var){
rowSums(ProbPred_integrated[, variable_names == var, drop=F])
})
par(mfrow = c(3, 4))  # create a 3 x 3 plotting matrix
for (j in 1 : ncol(X_original)) {
plot(X_original[,j], ProbPred_integrated[,j],
xlab = paste0("X", j), ylab = paste0("estimated propensity"))
}
# W <- 1 / .clipped(ProbPred)
W <- 1 / ProbPred * .overlap_weighting_g(ProbPred)
# W <- 1 / ProbPred * .entropy_weighting_g(ProbPred)
par(mfrow = c(4, 3))
for (j in 1 : ncol(X_original)) {
# plot(X_original[,j], .overlap_weighting_g(ProbPred_integrated[,j]),
#      xlab = paste0("X", j), ylab = paste0("OW"))
plot(X_original[,j], 1/ProbPred_integrated[,j] *.overlap_weighting_g(ProbPred_integrated[,j]),
xlab = paste0("X", j), ylab = paste0("OW"))
# plot(X_original[,j], .entropy_weighting_g(ProbPred_integrated[,j]),
#      xlab = paste0("X", j), ylab = paste0("EW"))
}
res_cl <- clamp_categorical(X=X_original, y=Y, W=W,
maxL = 5,
max_iter = 20,
nboots = 100,
seed = 123,
verbose = T)
plot(res_cl$level_pip)
plot(res_cl$variable_pip)
plot(res_cl$elbo, type = "o")
plot(res_cl$loglik, type = "o")
clamp_summarize_coefficients(res_cl)
summary(res_cl)
res_su <- susieR::susie(X_original, Y, L = 5, max_iter = 500)
res_su$pip
susieR::summary.susie(res_su)
res_cl$variable_pip
resY <- residuals(lm(Y ~ U))
res_su <- susieR::susie(X_original, resY, L = 5, max_iter = 500)
res_su$pip
susieR::summary.susie(res_su)
View(X_original[,c(3, 5)])
corrplot::corrplot(cor(X > 0), method = 'number')
corrplot::corrplot(cor(X_original > 0), method = 'number')
