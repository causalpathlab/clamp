---
title: "Finding the causal variables using a Poisson regression based CLAMP model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{clamp-poisson-causal}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>",
  fig.width = 5, fig.height = 4, fig.align = "center", fig.cap = "&nbsp;", 
  dpi = 120
)
```

```{r setup, message=FALSE}
# library(clamp)
devtools::load_all(".")
library(matrixStats)
library(corrplot)
```



## Toy example 1: Synthetic data where input data $\mathbf{X}$ is (somehow) highly correlated

Here is a simple example:

Let $p=10$ and $n=1000$.

We simulate each input variable (treatment) $X_j$ with

$$
X_{ij}=1 \sim \operatorname{Bern}(\operatorname{expit}(L^*_{ij})), 
$$

where we let the (observable) confounding factor $U \sim N(0, 1^2)$, and the ture latent confounding factors of each subject $U_{ij}$ to be

$$
u^*_{ij}=\zeta_j u_i +\delta_{ij}, \ \delta_{ij} \sim N(0, 0.5^2)
$$

and the scalar $\zeta_j = (j-5.5)/6$.

For $j=1, \ldots, 10$, we let

$$
g_j(t) = \frac{1}{1+\exp\left(- t\right)}
$$

This is to let logistic regression model catch the correct propensity score.

The binary response $y$ is generated with

$$
y_i|(\mathbf{x}_i, u_i) \sim \operatorname{Poisson}(\lambda_i),
$$

where 

$$
\lambda_i = \operatorname{exp}(\operatorname{scale}(\eta_i)), 
$$

$$
\eta_i = \beta_{1} x_{i1} + \beta_{4} x_{i4} + \beta_{9} x_{i9} + \theta_{u} u_i
$$

where regression coefficients $\beta_1, \beta_4, \beta_9, \theta_u$ are 1 for simplicity. 
The linear predictor $\boldsymbol{\eta}=(\eta_1, \cdots, \eta_n)$ is scaled 
to avoid unexpected "outliers". 

```{r}
set.seed(104104)
nn <- 1000
pp <- 100
U <- rnorm(nn)
zeta <- ((1:pp) -5.5) / 6
hist(zeta)
delta <- matrix(rnorm(nn*pp, 0, 0.1), nrow=nn)
UU <- outer(U, zeta) + delta

Pmat <- expit(UU)  # Prob matrix

X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)

causal_vars <- c(1, 4, 9)
# causal_vars <- c(1, 2, 10)

## output
# coefs <- rnorm(length(causal_vars) + 1)
coefs <- rep(1, times = length(causal_vars) + 1)
print(data.frame(variable = c(paste0("X", causal_vars), "U"), 
                 effect_size = coefs))

Eta <- cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs)
y <- rpois(nn, exp(Eta))
```

```{r}
hist(log1p(y))
```

Check the probability matrix:

```{r}
# Check the probability matrix
hist(as.numeric(Pmat), main = "propensity")
# apply(Pmat, 2, function(x){sum(x<= 0.1 | x >= 0.9)})
range(Pmat)
```

```{r}
corrplot(cor(Pmat), method = "number")
```

### Step 1: estimate the propensity scores and construct the weight matrix

```{r estimate ps}
PS <- apply(X, 2, 
            function(xcol) predict(glm(xcol ~ U, family = binomial), type = "response"))

Wmat <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(Wmat)
```

Check the propensity score matrix

```{r}
# Check the probability matrix
hist(as.numeric(PS), main = "Estimated propensity")
# apply(PS, 2, function(x){sum(x<= 0.1 | x >= 0.9)})
range(PS)
```

### Step 2: fit the clamp model

```{r}
res_cl <- clamp(X, y, W=Wmat, family = "poisson", 
                standardize = F,
                estimate_residual_variance = T,
                tol = 1e-2)

clamp_plot(res_cl, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl)
```

```{r}
summary(res_cl)
```

```{r}
clamp_plot(res_cl, y = "PIP", effect_indices = causal_vars)
```

### Apply the Huber reweighting approach for robust estimation:

```{r}
for( b in 1:100 ){
idx <- sample(nrow(X), replace=T)

res_cl_hb <- clamp(X[idx, , drop = F], y[idx], W=Wmat[idx, , drop=F], 
                   family = "poisson", 
                   standardize = F,
                   estimate_residual_variance = F,
                   robust_method = "huber",
                   maxL = 2,
                   tol = 1e-2,
                   estimate_prior_variance = T)

summarize_coefficients(res_cl_hb)
}
```

```{r}
summarize_coefficients(res_cl_hb)
```

```{r}
clamp_plot(res_cl_hb, y = "PIP", effect_indices = causal_vars)
```





## Comparison: What if the confounder is not adjusted?

```{r}
res_cl_v0 <- clamp(X, y, family = "poisson", 
                   standardize = F, 
                   estimate_residual_variance = F,
                   tol = 1e-2)
```

```{r}
summarize_coefficients(res_cl_v0)
```




