---
title: "IPW in Poisson regression"
author: "Ming Yuan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all(".")
```

## A simple example: 

```{r}
expit <- function(x) { 
  return(ifelse(x > 0, 1 / (1 + exp(-x)), exp(x) / (1 + exp(x))))
  }
```

```{r}
nn <- 1000
U <- rnorm(nn)
Prob <- expit(2 * U) 
# hist(Prob)

X <- rbinom(nn, 1, Prob)
Eta <- X * 1 + U * 1 + 1
Y <- rpois(nn, exp(Eta))
```

```{r}
hist(log1p(Y))
```

## Check whether inverse probability weighting can find out the true causal effect

```{r}
PS <- predict(glm(X ~ U, family = binomial), type = "response")
# plot(PS, Prob)
W <- ifelse( X == 1, 1/PS, 1/(1-PS) )
```


```{r}
estEY0 <- mean( (X==0) * (Y*W) ) / mean( (X==0) * W )
estEY1 <- mean( (X==1) * (Y*W) ) / mean( (X==1) * W )
estEY0
estEY1
estEY1 - estEY0
```

```{r}
res_glm <- glm(Y ~ X, family = poisson, weights = W)
summary(res_glm)
```

Thus, 

```{r}
est_coefs <- coefficients(res_glm)
exp(est_coefs[1] + est_coefs[2]) - exp(est_coefs[1])
```



#### Using the CLAMP 

```{r}
res_cl <- clamp(as.matrix(as.numeric(X)), Y, W = W, family = "poisson",
                # standardize = F,
                estimate_residual_variance = F)
summarize_coefficients(res_cl)
```


```{r, eval = F}
B <- 1000

glm_coefs_boot <- matrix(nrow = B, ncol = 2)
# clp_coefs_boot <- list(coefs = matrix(nrow = B, ncol = 2),
#                        scale_factors = matrix(nrow = B, ncol = 2))

for (b in 1 : B) {
  bootidx <- sample.int(nn, size = nn, replace = T)
  
  Xboot <- X[bootidx]
  Uboot <- U[bootidx]
  Yboot <- Y[bootidx]
  PSboot <- predict(glm(Xboot ~ Uboot, family = binomial), type = "response")
  Wboot <- ifelse( Xboot == 1, 1/PSboot, 1/(1-PSboot) )
  
  res_glm_boot <- glm(Yboot ~ Xboot, family = poisson, weights = Wboot)
  glm_coefs_boot[b, ] <- coefficients(res_glm_boot)
  
  # res_cl_boot <- clamp(as.matrix(as.numeric(Xboot)), Yboot, W = as.matrix(Wboot),
  #                      family = "poisson")
  # clp_coefs_boot[[1]][b, ] <- clamp_get_posterior_mean(res_cl_boot)
  # clp_coefs_boot[[2]][b, ] <- res_cl_boot$X_column_scale_factors
}


colnames(glm_coefs_boot) <- c("(Intercept)", "X")
apply(glm_coefs_boot, 2, quantile, probs = c(0.025, 0.975))

# colnames(clp_coefs_boot[[1]]) <- c("X", "(Intercept)")
# apply(clp_coefs_boot[[1]], 2, quantile, probs = c(0.025, 0.975))
```


## Short conclusion:

Assume that the poisson model is given by 
$$
Y_i \sim \operatorname{Poisson}(\lambda_i), 
$$

where 

$$
\lambda_i := \lambda(X_i, U_i) = \exp (\beta_0 + \beta_1 X_i + \beta_2 U_i ). 
$$

Suppose $X$ is a binary treatment. If we denote

$$
\hat{E}[Y(x)] = \frac{\sum_{i \in I\{X_i = x\}}Y_i \hat{W}_i}
                     {\sum_{i \in I\{X_i=x\}} \hat{W}_i},
$$

Conventionally, the difference of the estimated expectation of potential outcomes

$$
\operatorname{ATE} :=\hat{E}[Y(1)] - \hat{E}[Y(0)] 
$$

is used to estimate the "average treatment effect (ATE)". 

For linear models without interactions, i.e., 

$$
Y(x, u) = \beta_0 + \beta_1 X + \theta U + \epsilon, \ \epsilon \sim N(0, \sigma^2), 
$$

we know that $\operatorname{ATE} = \beta_1$. 

For **non-linear models**, we should not expect that the estimated ATE 
equals the regression coefficient. 
For example, for Poisson regression models, 

$$
\mathbb{E}[Y(x, u)] = \exp (\beta_0 + \beta_1 X + \theta U), 
$$

the corresponding ATE is 

$$
\operatorname{ATE} 
= \mathbb{E}_U \{\mathbb{E}[Y(1, U)] - \mathbb{E}[Y(0, U)]\}
= \int_u \left[\exp (\beta_0 + \beta_1 + \theta u) - 
\exp(\beta_0 + \theta u) \right] p(u) du. 
$$

Is this correct?!

<!-- Check the example above by Monte Carlo integration (??!):  -->
<!-- ```{r} -->
<!-- uu <- rnorm(10000) -->
<!-- mean(exp(1 + 2 + 1 * uu) - exp(1 + 1 * uu)) -->
<!-- ``` -->









