---
title: "Finding causal variables using a linear regression based CLAMP model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{clamp-linear-causal}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>",
  fig.width = 5, fig.height = 4, fig.align = "center", fig.cap = "&nbsp;", 
  dpi = 120
)
```

```{r setup}
# library(clamp)
devtools::load_all(".")
library(matrixStats)
library(corrplot)
library(data.table)
```

```{r}
library(rsvd)
library(susieR)
library(glmnet)
library(knockoff)
```


## An example: synthetic data

This is a simulated data set `X` generated by random numbers. 

```{r synthetic data}
set.seed(107109)
nn <- 500
# pp <- 1000
pp <- 200
h2 <- 0.3
U <- rnorm(nn)
zeta <- ( (1:pp) - pp/2 ) / (pp+1)
# zeta <- rnorm(pp)
# hist(zeta)
delta <- matrix(rnorm(nn*pp, 0, 0.01), nrow=nn)
UU <- (outer(U, zeta) + delta)[, , drop=F]

Pmat <- expit(UU)  # Prob matrix
# hist(cor(Pmat), breaks = seq(-1, 1, by = 0.1))
# corrplot(cor(Pmat), type = "upper")

X <- sapply(1:pp, function(j) {rbinom(nn, 1, Pmat[,j])})
X <- apply(X, 2, as.double)

esp <- rnorm(nn)
causal_vars <- sample.int(pp, size = 5)

# coefs <- rnorm(length(causal_vars) + 1)
coefs <- append(rep(1, times = length(causal_vars)), 1)
y <- sqrt(h2) * (cbind(X[, causal_vars, drop=F], U) %*% as.matrix(coefs)) + 
  sqrt(1-h2) * esp

print(data.frame(variable = c(paste0("X", causal_vars), "U"), 
                 effect_size = coefs))
```



### Step 1: estimate the propensity scores and construct the weight matrix

```{r}
Uhat <- rsvd(X, k=3)$u
PS <- sapply(1:ncol(X),
            function(j) predict(glm(X[,j] ~ Uhat, family = binomial), type = "response"))
```

```{r}
Wmat <- ifelse(X == 1, 1/PS, 1/(1-PS))
range(Wmat)
```

## Step 2: fit a clamp model

```{r}
res_cl1 <- clamp(X, y, W=Wmat, 
                mle_estimator = "mHT",
                max_iter = 20,
                maxL = 10,
                seed = 123, 
                verbose = T)

clamp_plot(res_cl1, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl1)
```

```{r}
plot(res_cl1$elbo, type = "l")
```

```{r}
compute_avg_elbos <- function(.elbos, burn_in=10, converge_window = 10) {
  .elbos <- .elbos[-(1:burn_in)]  
  avg_elbo <- rep(NA, times = length(.elbos) - converge_window)
  for (i in 1 : (length(.elbos) - converge_window)) {
    endpoint <- i + converge_window
    avg_elbo[i] <- mean(.elbos[i:endpoint])
  }
  return(avg_elbo)
}

avg_elbo <- compute_avg_elbos(res_cl1$elbo)
plot(avg_elbo, type = "l")
```


```{r}
res_cl3 <- clamp(X, y, W=Wmat, 
                standardize = F,
                mle_estimator = "WLS",
                max_iter = 10,
                maxL = 10,
                seed = 123, 
                verbose = T)

clamp_plot(res_cl3, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl3)
```

```{r}
res_cl2 <- clamp(X, y, W=Wmat, 
                standardize = T,
                mle_estimator = "WLS",
                max_iter = 20,
                maxL = 10,
                seed = 123, 
                verbose = T)

clamp_plot(res_cl2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_cl2)
```


```{r}
plot(res_cl1$elbo, type = "o")
plot(res_cl1$loglik, type = "o")
```


> **Sample size issue**: when the sample size $n$ is small, e.g., $n\leq 100$, it is very likely that the algorithm raises an error or outputs nothing if specifying `mle_variance_estimator="bootstrap"`, especially if further specifying `standardize=T`. This is because the bootstrap variance using the scaled input $\mathbf{X}$ tends to be (much) larger than expected, then correspondingly, the PIPs are relatively small (a sort of conservative estimation). Besides, the target function of optimizing the prior variance (`prior_varB`) is proportional to the sum of Bayes Factors, which is not a convex function of `prior_varB`. 

### Compare with `susieR`

```{r}
res_su <- susieR::susie(X, y)
summarize_coefficients(res_su)
clamp_plot(res_su, y = "PIP", effect_indices = causal_vars)
# plot_select_results(res_su$pip, effect_idx = causal_vars) + ylab("PIP")
```

```{r}
yres <- residuals(lm(y ~ Uhat))
res_su2 <- susieR::susie(X, yres)
clamp_plot(res_su2, y = "PIP", effect_indices = causal_vars)
summarize_coefficients(res_su2)

# plot_select_results(res_su2$pip, effect_idx = causal_vars) + ylab("PIP")
```


### Compare with LASSO

```{r, message=F, warning=F}
library(tidyverse)
myPalette2 <- c("#E69F00", "#999999", "#00757F")
plot_select_results <- function(vals, effect_idx) {
  tibble(vals = vals) %>%
    mutate(id = 1 : n()) %>%
    mutate(true_label = if_else(id %in% effect_idx, "causal", "noncausal")) %>%
    arrange(desc(true_label)) %>%
    ggplot(aes(x = id, y = vals, color = as.factor(true_label))) +
    geom_point(size = 3) +
    scale_color_manual(values = myPalette2[1:2]) +
    labs(x = "variable", color = "") +
    theme_classic()
}
```


```{r}
res_la <- cv.glmnet(X, y, family = "gaussian", alpha = 1)

plot_select_results(abs(as.numeric(coef(res_la, s = "lambda.min"))[-1]), 
                    effect_idx = causal_vars) +
  ylab("|coefficients|")

out <- data.frame(variable = paste0("X", 1:pp), 
           Est = coef(res_la, s = "lambda.min")[-1]) %>%
  arrange(desc(abs(Est)))
print.data.frame(out[1:10, ])
```

### Compare with Gaussian-Knockoff

```{r}
res_ko <- knockoff.filter(X, y)
plot_select_results(res_ko$statistic, causal_vars) + 
  geom_hline(yintercept = res_ko$threshold, 
             linetype = 5, size = 1.5, colour = myPalette2[3])
```


## Another example: genotype data

```{r}
## Check the frequencies of alleles
count_allele_freqs <- function(Xcol) {
  genotype_freq_tb <- t(as.matrix(table(Xcol)))
  
  if ( !("0" %in% colnames(genotype_freq_tb)) ) 
    { genotype_freq_tb <- cbind("0" = 0, genotype_freq_tb) }
  if ( !("1" %in% colnames(genotype_freq_tb)) ) 
    { genotype_freq_tb <- cbind("1" = 0, genotype_freq_tb) }
  
  allele_freq_tb <- matrix(NA, ncol = 2)
  allele_freq_tb[,1] <- 
    (2*genotype_freq_tb[,"0"] + 1*genotype_freq_tb[,"1"]) / (2*length(Xcol)) 
  allele_freq_tb[,2] <- 1 - allele_freq_tb[,1]
  
  return(allele_freq_tb)
}
```


This is a genotype data set `X`, where the initial entries are either 0, 1, or 2.

```{r genotype data}
nn <- 500
pp <- 100
startpoint <- 200
# Xorg <- readRDS("./example-data/genotype-subset-1.rds")[1:nn, 1:pp]
X <- readRDS("./example-data/genotype-subset-1.rds")[1:nn, startpoint:(startpoint-1+pp)]
colnames(X) <- paste0("X", 1:pp)

Allele_Freqs <- t(apply(X, 2, count_allele_freqs))
MAF <- apply(Allele_Freqs, 1, min)
which(MAF < 0.05)

X <- X[, -which(MAF < 0.05), drop=F]
dim(X)
```

Before generating Y, remove the SNPs whose minor allele frequencies (`MAF`) 
are less than 0.05.

```{r}
set.seed(12345)
h2 <- 1
esp <- rnorm(nn)
causal_vars <- sample(colnames(X), size = 3)
coefs_causal <- rep(1, times = length(causal_vars))

## substitute confounders: the first 3 PCs of X
Uhat <- rsvd(scale(X), k=3)$u  
coefs_cf<- rep(3, times = ncol(Uhat))
# coefs_cf <- rnorm(ncol(Uhat), sd=0.5)

## supposing X is additive. 
y <- sqrt(h2) * (X[, causal_vars, drop=F] %*% as.matrix(coefs_causal) + 
                 Uhat %*% as.matrix(coefs_cf) ) + sqrt(1-h2) * esp

print(data.frame(variable    = c(causal_vars, paste0("Uhat", 1:ncol(Uhat))), 
                 coefficient = c(coefs_causal, coefs_cf)))
```


### Step 0: reconstruct an augmented `Xaug`

Here, the augmented X, named `Xaug`, has a column size 3 times of the original 
`X`, where each column of X is expanded into three columns: 0 vs 1&2, 1 vs 0&2,
and 2 vs 0&1.

This encoding method similar to the "one-hot encoding". 
The difference is that the one-hot encoding method uses 1 to represents the 
specific category, whereas our encoding method uses 0. 
Besides, the one-hot encoding method 

```{r}
X_base0 <- ifelse(X == 0, 0, 1)
colnames(X_base0) <- paste0(colnames(X), "_b0")

# X_base1 <- ifelse(X == 1, 0, 1)
# colnames(X_base1) <- paste0(colnames(X), "_b1")
# 
# X_base2 <- ifelse(X == 2, 0, 1)
# colnames(X_base2) <- paste0(colnames(X), "_b2")

# Xaug <- cbind(X_base0, X_base1, X_base2)
# # Xaug <- cbind(X_base0, X_base2)
Xaug <- X_base0

## Check the class frequencies
class1freq <- apply(Xaug, 2, mean)
plot(class1freq)
abline(h = c(0.1,0.9))

## Remove the indicators (variables) whose class frequencies are less than 0.1 or greater than 0.9
# Xaug <- Xaug[, -which(class1freq <= 0.1 | class1freq >= 0.9)]
sum(class1freq <= 0.1 | class1freq >= 0.9)
dim(Xaug)
```

### Step 1: estimate the propensity scores and construct the weight matrix of `Xaug`

```{r}
PS <- matrix(NA, nrow = nrow(Xaug), ncol = ncol(Xaug))
for (j in 1 : ncol(Xaug)) {
  fit <- NULL
  warn_msg <- NULL
  
  fit <- withCallingHandlers(
    glm(Xaug[,j] ~ Uhat, family = binomial),
    warning = function(w) {
      warn_msg <<- w$message  # capture the warning message
      message(sprintf("Warning in column %d: %s", j, w$message))
      invokeRestart("muffleWarning")
    }
  )
  
  PS[,j] <- predict(fit, type = "response")
}

## check the estimated PS
# plot(Xaug[, 18], PS[, 18])
```

```{r}
# crop "extreme" propensity scores (< 0.05 or > 0.95).
crop_ps <- function(ps, tol = 0.05) {
  ps <- ifelse(ps < tol, tol, ps) 
  ps <- ifelse(ps > 1-tol, 1-tol, ps)
}
PS <- crop_ps(PS)

Wmat <- ifelse(Xaug == 1, 1/PS, 1/(1-PS))
# range(Wmat)
```

For some columns, they can be perfectly separated by the first or second PC, 
and then the `glm()` function will raise up a warning claiming that 
"glm.fit: algorithm did not converge". 
For some other columns, a subset of the data is perfectly separable based on 
the predictors, while the rest is not (i.e., quasi-separation). 

### Step 2: Fit a CLAMP model

```{r}
res_cl1 <- clamp(X=Xaug, y=y, W=Wmat, 
                 mle_estimator = "mHT", 
                 maxL = 10,
                 estimate_prior_variance = F, 
                 max_iter = 100,
                 seed = 123, 
                 verbose = T)
```

```{r}
clamp_plot(res_cl1, y = "PIP", 
           effect_indices = 
             which(colnames(Xaug) %in% c(paste0(causal_vars, "_b0"),
                                         paste0(causal_vars, "_b1"),
                                         paste0(causal_vars, "_b2"))) )  
causal_vars
summarize_coefficients(res_cl1)
```

```{r}
compute_avg_elbos <- function(.elbos, burn_in=10, converge_window = 10) {
  .elbos <- .elbos[-(1:burn_in)]  
  avg_elbo <- rep(NA, times = length(.elbos) - converge_window)
  for (i in 1 : (length(.elbos) - converge_window)) {
    endpoint <- i + converge_window
    avg_elbo[i] <- mean(.elbos[i:endpoint])
  }
  return(avg_elbo)
}

avg_elbo <- compute_avg_elbos(res_cl1$elbo)
plot(avg_elbo, type = "l")
plot(res_cl1$elbo, type = "l")
```


There are still a lot of null variables being selected.


## Compare with SuSiE

```{r}
# res_su <- susieR::susie(Xaug, y, max_iter = 500)
# clamp_plot(res_su, y = "PIP",
#            effect_indices = which(colnames(Xaug) %in% c(paste0(causal_vars, "_b0"),
#                                                         paste0(causal_vars, "_b1"),
#                                                         paste0(causal_vars, "_b2"))))

res_su <- susieR::susie(X, y,max_iter = 100)
clamp_plot(res_su, y = "PIP",
           effect_indices = which(colnames(X) %in% causal_vars))
summarize_coefficients(res_su)
causal_vars
# summary(res_su)
```

